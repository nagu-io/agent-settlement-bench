Writing
AgentSettlementBench v1.0

Initial public release of AgentSettlementBench â€” a benchmark designed to evaluate whether AI agents can safely make irreversible financial decisions in cryptocurrency payment settlement environments.

What this benchmark tests

RPC consensus disagreement

Chain reorganization risks

Replay attacks

Incorrect recipient settlement

Race conditions

Finality uncertainty

Why this exists

Traditional LLM benchmarks measure reasoning ability in static problems.
This benchmark evaluates safety behavior in adversarial distributed systems where incorrect decisions cause permanent loss.

Key Finding

Model safety depends strongly on instruction structure and system architecture rather than raw intelligence.

Open reasoning models show significant failure rates in edge-case operational logic such as consensus disagreement and timing races.

Intended Use

AI agent evaluation

Financial automation safety research

Alignment and reliability testing

Distributed system decision boundaries

Notes

This release establishes the baseline dataset and scoring methodology.
Future releases will expand adversarial coverage and real-trace replay testing.
